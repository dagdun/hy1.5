# HunyuanVideo-1.5 Docker + RunPod Handler

This repository now contains a reproducible Docker image that boots the official Tencent HunyuanVideo-1.5 codebase, pre-downloads all checkpoints, and exposes a RunPod Serverless handler that mirrors the upstream `generate.py` command-line interface while returning base64-encoded video data.

## Build Instructions

1. Obtain a Hugging Face token with access to `black-forest-labs/FLUX.1-Redux-dev` (required for the SigLIP encoder).
2. Build the image while passing the token (any of `HF_TOKEN`, `HY_SIGLIP_TOKEN`, `HUGGINGFACE_TOKEN`, or `RUNPOD_HF_TOKEN` workâ€”pick whichever is most convenient for your CI/CD or RunPod secret wiring):

   ```bash
   docker build \
     --build-arg HF_TOKEN=hf_your_token_here \
     -t hunyuan-video:1.5 .
   ```

   The build runs `setup_hunyuan.sh`, installs all Python dependencies inside `/opt/hunyuan/.venv`, clones the upstream repository into `/opt/hunyuan/HunyuanVideo-1.5`, and downloads every checkpoint into `/opt/hunyuan/HunyuanVideo-1.5/ckpts`.

   On RunPod Hub, add a build secret named (for example) `RUNPOD_HF_TOKEN` and wire it through `.runpod/hub.json` or the dashboard as a Docker build argument:

   ```jsonc
   {
     "build": {
       "args": {
         "RUNPOD_HF_TOKEN": "${secrets.RUNPOD_HF_TOKEN}"
       },
       "secrets": [
         {
           "name": "RUNPOD_HF_TOKEN",
           "description": "Hugging Face token with access to FLUX.1-Redux-dev"
         }
       ]
     }
   }
   ```

   Any one of the supported argument names ultimately becomes `HY_SIGLIP_TOKEN` for `setup_hunyuan.sh`, so pick whichever naming scheme matches your platform.

## Local Testing

The image defaults to starting the RunPod handler. To run ad-hoc commands inside the container (e.g., quickly test `generate.py`), override the entrypoint:

```bash
docker run --rm -it --gpus all \
  --entrypoint /bin/bash \
  hunyuan-video:1.5
# Inside the container:
source /opt/hunyuan/.venv/bin/activate
cd /opt/hunyuan/HunyuanVideo-1.5
python generate.py --help
```

You can also reuse the included `infer.sh` after setting `PROMPT`, `IMAGE_PATH`, etc.

## RunPod Serverless Handler

The `runpod_handler.py` module mirrors `generate.py` parameters. Submit JSON payloads whose `input` field matches the CLI arguments documented in the upstream README, e.g.:

```jsonc
{
  "input": {
    "prompt": "A macro shot of dewdrops rolling down a leaf.",
    "resolution": "480p",
    "num_inference_steps": 20,
    "cfg_distilled": true,
    "sr": false,
    "model_path": "/opt/hunyuan/HunyuanVideo-1.5/ckpts",
    "image_base64": "<optional base64 for i2v>",
    "seed": 34344945
  }
}
```

Supported fields:

- All flags listed in [`generate.py`](https://github.com/Tencent-Hunyuan/HunyuanVideo-1.5/blob/main/generate.py), e.g. `negative_prompt`, `aspect_ratio`, `video_length`, `sparse_attn`, cache-related knobs, etc.
- Optional `image_base64` lets you submit an inline PNG/JPEG for image-to-video mode. When omitted, `image_path` defaults to `None` and the handler produces text-to-video samples.

The handler returns:

- `video_base64`: base64-encoded MP4 bytes generated by `generate.py`.
- `stdout` / `stderr`: logs captured from the generation run.
- `config_json`: if `--save_generation_config` is left enabled, the serialized configuration also ships back as a string for downstream inspection.

### Notes

- `model_path` defaults to `/opt/hunyuan/HunyuanVideo-1.5/ckpts` inside the container, so you generally do not need to override it when running on RunPod.
- Every invocation writes artifacts to a private temp directory that is cleaned automatically after encoding the MP4 to base64.
- Non-boolean CLI values are passed through verbatim, so you can fine tune inference behavior without changing the handler.
